{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn, pennylane, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing kernel methods and variational quantum circuits on a real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\n",
    "from pennylane.operation import Tensor\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a random number generator for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preparing the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# pick only the first two classes (corresponding to the first 100 samples)\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "# scaling the data for better training and avoiding problems due to periodic encoding\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# move labels from [0, 1] to [-1, 1] range\n",
    "y_scaled = 2 * (y - 0.5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact amplitude encoding may require up to an exponential amount of operations and thus it does not work for NISQ. We can do another type of encoding, i.e. *angle encoding*, for which the features are encoded in as many qubits and each of them is used as a rotation angle of a gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function that encodes each of the features as a $R_Z$ rotation for each qubit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_encoding_rz(x):\n",
    "    \"\"\"Encodes features as Z-rotations\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray): data point where the entries are the features to be encoded\n",
    "    \"\"\"\n",
    "    # ================\n",
    "    # YOUR CODE BELOW\n",
    "    # ================\n",
    "    for i, xx in enumerate(x):\n",
    "        qml.RZ(xx, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In quantum kernel methods, the quantum computer is used exclusively to compute the kernel matrix. \n",
    "\n",
    "Remember that the kernel is given by the inner product of two samples $x_1$ and $x_2$. More correctly, it is the inner product between their mappings in the higher-dimensional feature space, thus $\\phi(x_1)$ and $\\phi(x_2)$. For us, this higher-dimensional mapping is given by our $R_Z$-encoding, which leads to the states $|\\phi(x_1)\\rangle$ and $|\\phi(x_2)\\rangle$. Therefore, we want to compute $|\\langle \\phi(x_2) | \\phi(x_1) \\rangle|^2$ (the square matters because the kernel is a measure of a distance).\n",
    "\n",
    "It is easy to verify that the circuit used to compute the kernel is\n",
    "\n",
    "<img src=\"kernel.png\" alt=\"kernel\" width=\"400\"/>\n",
    "\n",
    "where $\\Phi(x)$ computes the feature map and the final measuerement is on the all-zeros projector $|0\\dots 0\\rangle\\langle 0\\dots 0|$ (equivalent to computing the probability of measuring the all-zeros state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_1 = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "projector = np.zeros((2 ** n_qubits, 2 ** n_qubits))\n",
    "projector[0, 0] = 1\n",
    "\n",
    "@qml.qnode(dev_1)\n",
    "def kernel(x1, x2):\n",
    "    \"\"\"Computes the kernel with the R_Z feature map\n",
    "\n",
    "    Args:\n",
    "        x1 (np.ndarray): first sample\n",
    "        x2 (np.ndarray): second sample\n",
    "\n",
    "    Returns:\n",
    "        float: kernel value\n",
    "    \"\"\"\n",
    "    AngleEmbedding(x1, wires=range(n_qubits))\n",
    "    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n",
    "    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel(X_train[0], X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a way to compute the kernel, we can use it to train a support vector machine, which is a classifier. This requires to solve an optimization problem to determine the parameters of the so-called *dual-problem*, which in tuern allow to make predictions for new samples.\n",
    "\n",
    "This procedure is easily taken care of by the scikit-learn library, provided that we build a function for the kernel **matrix**. This has a syntax where the two set of samples can belong to two different datasets A and B, even though, in our case A=B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(A, B):\n",
    "   \"\"\"Returns the kernel matrix from entries in two datasets\n",
    "\n",
    "   Args:\n",
    "       A (List[np.ndarray]): first dataset\n",
    "       B (List[np.ndarray]): second dataset\n",
    "\n",
    "   Returns:\n",
    "       np.ndarray: kernel matrix\n",
    "   \"\"\"\n",
    "   return np.array([[kernel(a, b) for b in B] for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=kernel_matrix).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained SVM can now be used on unseen samples (test data-set) to see how it performs in predicting their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = svm.predict(X_test)\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with a variational classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the parity training, here we will be using a variational quantum classifier to classify the Iris dataset. After training, we will check the accuracy score and compare it with the kernel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev_1)\n",
    "def vqc_circuit(theta, x):\n",
    "    \"\"\"Returns the variational quantum circuit with angle embedding and strongly entangling layers\n",
    "    as ansatz\n",
    "\n",
    "    Args:\n",
    "        theta (tensor_like): trainable ansatz parameters. The first dimension determines the number of\n",
    "            layers. Shape = (L, M ,3); L = layers, M = n_qubits, 3 = rotations per qubit.\n",
    "        x (np.ndarray): sample (properly scaled)\n",
    "\n",
    "    Returns:\n",
    "        float: prediction of the quantum model\n",
    "    \"\"\"\n",
    "\n",
    "    # embedding\n",
    "    AngleEmbedding(x, wires=range(n_qubits))\n",
    "\n",
    "    # trainable measurement\n",
    "    StronglyEntanglingLayers(theta, wires=range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqc_with_bias(theta, bias, x):\n",
    "    \"\"\"Adds bias to the outcome of a vqc\n",
    "\n",
    "    Args:\n",
    "        theta (tensor_like): trainable ansatz parameters\n",
    "        bias (float): classical bias\n",
    "        x (np.ndarray): sample (properly scaled)\n",
    "\n",
    "    Returns:\n",
    "        float: (vqc output) + bias\n",
    "    \"\"\"\n",
    "    return vqc_circuit(theta, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    \"\"\"Computes the MSE between labels and predictions\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): actual values\n",
    "        predictions (List[int]): model predictions\n",
    "\n",
    "    Returns:\n",
    "        float: value of the MSE\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l - p) ** 2\n",
    "\n",
    "    loss /= len(labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, bias, X, y):\n",
    "    \"\"\"Computes the predictions and returns the MSE over the dataset\n",
    "\n",
    "    Args:\n",
    "        theta (List[np.ndarray]): all the variational parameters, one\n",
    "            array per layer\n",
    "        bias (float): classical bias value, added to the output of the quantum circuit\n",
    "        X (List[List]): list of all train samples\n",
    "        y (List): labels\n",
    "\n",
    "    Returns:\n",
    "        float: MSE over the dataset\n",
    "    \"\"\"\n",
    "    predictions = [vqc_with_bias(theta, bias, x) for x in X]\n",
    "    return square_loss(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6\n",
    "n_iters = 100\n",
    "\n",
    "theta_init = .01 * rng.normal(size=(n_layers, n_qubits, 3), requires_grad=True)\n",
    "bias_init = np.array(0., requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.0698052 | Acc train: 0.4666667 | Acc validation: 0.3600000 \n",
      "Iter:     2 | Cost: 1.0664597 | Acc train: 0.4533333 | Acc validation: 0.3200000 \n",
      "Iter:     3 | Cost: 1.0648884 | Acc train: 0.4666667 | Acc validation: 0.2800000 \n",
      "Iter:     4 | Cost: 1.0663347 | Acc train: 0.4533333 | Acc validation: 0.3200000 \n",
      "Iter:     5 | Cost: 1.0640832 | Acc train: 0.4800000 | Acc validation: 0.2800000 \n",
      "Iter:     6 | Cost: 1.0649951 | Acc train: 0.4666667 | Acc validation: 0.2800000 \n",
      "Iter:     7 | Cost: 1.0624800 | Acc train: 0.4800000 | Acc validation: 0.2800000 \n",
      "Iter:     8 | Cost: 1.0593197 | Acc train: 0.4666667 | Acc validation: 0.3600000 \n",
      "Iter:     9 | Cost: 1.0549767 | Acc train: 0.4666667 | Acc validation: 0.4000000 \n",
      "Iter:    10 | Cost: 1.0521434 | Acc train: 0.4933333 | Acc validation: 0.4000000 \n",
      "Iter:    11 | Cost: 1.0507942 | Acc train: 0.4666667 | Acc validation: 0.4800000 \n",
      "Iter:    12 | Cost: 1.0510021 | Acc train: 0.4933333 | Acc validation: 0.4800000 \n",
      "Iter:    13 | Cost: 1.0542901 | Acc train: 0.4933333 | Acc validation: 0.5200000 \n",
      "Iter:    14 | Cost: 1.0592541 | Acc train: 0.4666667 | Acc validation: 0.5200000 \n",
      "Iter:    15 | Cost: 1.0773474 | Acc train: 0.5200000 | Acc validation: 0.5200000 \n",
      "Iter:    16 | Cost: 1.1073588 | Acc train: 0.5466667 | Acc validation: 0.5600000 \n",
      "Iter:    17 | Cost: 1.1339355 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    18 | Cost: 1.1452410 | Acc train: 0.5066667 | Acc validation: 0.5600000 \n",
      "Iter:    19 | Cost: 1.1457178 | Acc train: 0.5066667 | Acc validation: 0.5600000 \n",
      "Iter:    20 | Cost: 1.1469956 | Acc train: 0.5200000 | Acc validation: 0.5600000 \n",
      "Iter:    21 | Cost: 1.1331195 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    22 | Cost: 1.1132684 | Acc train: 0.5466667 | Acc validation: 0.5600000 \n",
      "Iter:    23 | Cost: 1.0944085 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    24 | Cost: 1.0784506 | Acc train: 0.5466667 | Acc validation: 0.5600000 \n",
      "Iter:    25 | Cost: 1.0629347 | Acc train: 0.5066667 | Acc validation: 0.5600000 \n",
      "Iter:    26 | Cost: 1.0523653 | Acc train: 0.4666667 | Acc validation: 0.5200000 \n",
      "Iter:    27 | Cost: 1.0472465 | Acc train: 0.4933333 | Acc validation: 0.5200000 \n",
      "Iter:    28 | Cost: 1.0424531 | Acc train: 0.4933333 | Acc validation: 0.5600000 \n",
      "Iter:    29 | Cost: 1.0398083 | Acc train: 0.4933333 | Acc validation: 0.5200000 \n",
      "Iter:    30 | Cost: 1.0366787 | Acc train: 0.5066667 | Acc validation: 0.4800000 \n",
      "Iter:    31 | Cost: 1.0338573 | Acc train: 0.5066667 | Acc validation: 0.4800000 \n",
      "Iter:    32 | Cost: 1.0307342 | Acc train: 0.5066667 | Acc validation: 0.5200000 \n",
      "Iter:    33 | Cost: 1.0293423 | Acc train: 0.5200000 | Acc validation: 0.5600000 \n",
      "Iter:    34 | Cost: 1.0300222 | Acc train: 0.5200000 | Acc validation: 0.5200000 \n",
      "Iter:    35 | Cost: 1.0308190 | Acc train: 0.5200000 | Acc validation: 0.5600000 \n",
      "Iter:    36 | Cost: 1.0297475 | Acc train: 0.5466667 | Acc validation: 0.5600000 \n",
      "Iter:    37 | Cost: 1.0288612 | Acc train: 0.5466667 | Acc validation: 0.6000000 \n",
      "Iter:    38 | Cost: 1.0251152 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    39 | Cost: 1.0142184 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    40 | Cost: 1.0051543 | Acc train: 0.5333333 | Acc validation: 0.5600000 \n",
      "Iter:    41 | Cost: 0.9909220 | Acc train: 0.5333333 | Acc validation: 0.6000000 \n",
      "Iter:    42 | Cost: 0.9649034 | Acc train: 0.5733333 | Acc validation: 0.6000000 \n",
      "Iter:    43 | Cost: 0.9368777 | Acc train: 0.6133333 | Acc validation: 0.6000000 \n",
      "Iter:    44 | Cost: 0.8960784 | Acc train: 0.6533333 | Acc validation: 0.6400000 \n",
      "Iter:    45 | Cost: 0.8602368 | Acc train: 0.7333333 | Acc validation: 0.6400000 \n",
      "Iter:    46 | Cost: 0.8270337 | Acc train: 0.7200000 | Acc validation: 0.6400000 \n",
      "Iter:    47 | Cost: 0.7937087 | Acc train: 0.7333333 | Acc validation: 0.7200000 \n",
      "Iter:    48 | Cost: 0.7615481 | Acc train: 0.7866667 | Acc validation: 0.7200000 \n",
      "Iter:    49 | Cost: 0.7294536 | Acc train: 0.8133333 | Acc validation: 0.7600000 \n",
      "Iter:    50 | Cost: 0.6969540 | Acc train: 0.8666667 | Acc validation: 0.8000000 \n",
      "Iter:    51 | Cost: 0.6641648 | Acc train: 0.8800000 | Acc validation: 0.8000000 \n",
      "Iter:    52 | Cost: 0.6335874 | Acc train: 0.8800000 | Acc validation: 0.8000000 \n",
      "Iter:    53 | Cost: 0.6054489 | Acc train: 0.9066667 | Acc validation: 0.8400000 \n",
      "Iter:    54 | Cost: 0.5759825 | Acc train: 0.9066667 | Acc validation: 0.8400000 \n",
      "Iter:    55 | Cost: 0.5474100 | Acc train: 0.9466667 | Acc validation: 0.8400000 \n",
      "Iter:    56 | Cost: 0.5220633 | Acc train: 0.9733333 | Acc validation: 0.8400000 \n",
      "Iter:    57 | Cost: 0.4971162 | Acc train: 0.9733333 | Acc validation: 0.8400000 \n",
      "Iter:    58 | Cost: 0.4739897 | Acc train: 0.9733333 | Acc validation: 0.8800000 \n",
      "Iter:    59 | Cost: 0.4522539 | Acc train: 0.9733333 | Acc validation: 0.8800000 \n",
      "Iter:    60 | Cost: 0.4319561 | Acc train: 0.9733333 | Acc validation: 0.8800000 \n",
      "Iter:    61 | Cost: 0.4133514 | Acc train: 0.9733333 | Acc validation: 0.9200000 \n",
      "Iter:    62 | Cost: 0.3969378 | Acc train: 0.9733333 | Acc validation: 0.9200000 \n",
      "Iter:    63 | Cost: 0.3827253 | Acc train: 0.9733333 | Acc validation: 0.8800000 \n",
      "Iter:    64 | Cost: 0.3707566 | Acc train: 0.9866667 | Acc validation: 0.9200000 \n",
      "Iter:    65 | Cost: 0.3609882 | Acc train: 0.9866667 | Acc validation: 0.9200000 \n",
      "Iter:    66 | Cost: 0.3534657 | Acc train: 0.9733333 | Acc validation: 0.9600000 \n",
      "Iter:    67 | Cost: 0.3453438 | Acc train: 0.9733333 | Acc validation: 0.9600000 \n",
      "Iter:    68 | Cost: 0.3380093 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    69 | Cost: 0.3310651 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    70 | Cost: 0.3238843 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    71 | Cost: 0.3173447 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    72 | Cost: 0.3099944 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    73 | Cost: 0.3025964 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    74 | Cost: 0.2961524 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    75 | Cost: 0.2914518 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    76 | Cost: 0.2865384 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    77 | Cost: 0.2807431 | Acc train: 0.9600000 | Acc validation: 0.9600000 \n",
      "Iter:    78 | Cost: 0.2743169 | Acc train: 0.9733333 | Acc validation: 0.9600000 \n",
      "Iter:    79 | Cost: 0.2669367 | Acc train: 0.9733333 | Acc validation: 0.9600000 \n",
      "Iter:    80 | Cost: 0.2603855 | Acc train: 0.9733333 | Acc validation: 0.9600000 \n",
      "Iter:    81 | Cost: 0.2546733 | Acc train: 0.9733333 | Acc validation: 1.0000000 \n",
      "Iter:    82 | Cost: 0.2497894 | Acc train: 0.9866667 | Acc validation: 1.0000000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m acc_train \u001b[39m=\u001b[39m accuracy_score(y_train, predictions_train)\n\u001b[1;32m     21\u001b[0m acc_test \u001b[39m=\u001b[39m accuracy_score(y_test, predictions_test)\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mIter: \u001b[39m\u001b[39m{:5d}\u001b[39;00m\u001b[39m | Cost: \u001b[39m\u001b[39m{:0.7f}\u001b[39;00m\u001b[39m | Acc train: \u001b[39m\u001b[39m{:0.7f}\u001b[39;00m\u001b[39m | Acc validation: \u001b[39m\u001b[39m{:0.7f}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(it \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, cost(theta, bias, X_train, y_train), acc_train, acc_test)\n\u001b[1;32m     26\u001b[0m )\n",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m, in \u001b[0;36mcost\u001b[0;34m(theta, bias, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost\u001b[39m(theta, bias, X, y):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the predictions and returns the MSE over the dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m        float: MSE over the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     predictions \u001b[39m=\u001b[39m [vqc_with_bias(theta, bias, x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m square_loss(y, predictions)\n",
      "Cell \u001b[0;32mIn[30], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost\u001b[39m(theta, bias, X, y):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the predictions and returns the MSE over the dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m        float: MSE over the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     predictions \u001b[39m=\u001b[39m [vqc_with_bias(theta, bias, x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m square_loss(y, predictions)\n",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m, in \u001b[0;36mvqc_with_bias\u001b[0;34m(theta, bias, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvqc_with_bias\u001b[39m(theta, bias, x):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Adds bias to the outcome of a vqc\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m        float: (vqc output) + bias\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mreturn\u001b[39;00m vqc_circuit(theta, x) \u001b[39m+\u001b[39m bias\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/qnode.py:867\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecute_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    866\u001b[0m \u001b[39m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m res \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    868\u001b[0m     [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtape],\n\u001b[1;32m    869\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    870\u001b[0m     gradient_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_fn,\n\u001b[1;32m    871\u001b[0m     interface\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterface,\n\u001b[1;32m    872\u001b[0m     gradient_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_kwargs,\n\u001b[1;32m    873\u001b[0m     override_shots\u001b[39m=\u001b[39;49moverride_shots,\n\u001b[1;32m    874\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_kwargs,\n\u001b[1;32m    875\u001b[0m )\n\u001b[1;32m    877\u001b[0m res \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m]\n\u001b[1;32m    879\u001b[0m \u001b[39mif\u001b[39;00m old_interface \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/interfaces/execution.py:407\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_fn(res)\n\u001b[1;32m    405\u001b[0m \u001b[39mif\u001b[39;00m gradient_fn \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbackprop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m interface \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_fn(\n\u001b[0;32m--> 407\u001b[0m         qml\u001b[39m.\u001b[39;49minterfaces\u001b[39m.\u001b[39;49mcache_execute(\n\u001b[1;32m    408\u001b[0m             batch_execute, cache, return_tuple\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, expand_fn\u001b[39m=\u001b[39;49mexpand_fn\n\u001b[1;32m    409\u001b[0m         )(tapes)\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39m# the default execution function is batch_execute\u001b[39;00m\n\u001b[1;32m    413\u001b[0m execute_fn \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39minterfaces\u001b[39m.\u001b[39mcache_execute(batch_execute, cache, expand_fn\u001b[39m=\u001b[39mexpand_fn)\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/interfaces/execution.py:204\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[39mreturn\u001b[39;00m (res, []) \u001b[39mif\u001b[39;00m return_tuple \u001b[39melse\u001b[39;00m res\n\u001b[1;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     res \u001b[39m=\u001b[39m fn(execution_tapes\u001b[39m.\u001b[39;49mvalues(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m final_res \u001b[39m=\u001b[39m []\n\u001b[1;32m    208\u001b[0m \u001b[39mfor\u001b[39;00m i, tape \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tapes):\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/interfaces/execution.py:130\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(tapes: Sequence[QuantumTape], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     tapes \u001b[39m=\u001b[39m [expand_fn(tape) \u001b[39mfor\u001b[39;00m tape \u001b[39min\u001b[39;00m tapes]\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn(tapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/_qubit_device.py:588\u001b[0m, in \u001b[0;36mQubitDevice.batch_execute\u001b[0;34m(self, circuits)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mfor\u001b[39;00m circuit \u001b[39min\u001b[39;00m circuits:\n\u001b[1;32m    584\u001b[0m     \u001b[39m# we need to reset the device here, else it will\u001b[39;00m\n\u001b[1;32m    585\u001b[0m     \u001b[39m# not start the next computation in the zero state\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 588\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(circuit)\n\u001b[1;32m    589\u001b[0m     results\u001b[39m.\u001b[39mappend(res)\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mactive:\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/_qubit_device.py:318\u001b[0m, in \u001b[0;36mQubitDevice.execute\u001b[0;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_validity(circuit\u001b[39m.\u001b[39moperations, circuit\u001b[39m.\u001b[39mobservables)\n\u001b[1;32m    317\u001b[0m \u001b[39m# apply all circuit operations\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(circuit\u001b[39m.\u001b[39;49moperations, rotations\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_diagonalizing_gates(circuit), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    320\u001b[0m \u001b[39m# generate computational basis samples\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshots \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m circuit\u001b[39m.\u001b[39mis_sampled:\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/devices/default_qubit.py:276\u001b[0m, in \u001b[0;36mDefaultQubit.apply\u001b[0;34m(self, operations, rotations, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_parametrized_evolution(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state, operation)\n\u001b[1;32m    275\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_operation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_state, operation)\n\u001b[1;32m    278\u001b[0m \u001b[39m# store the pre-rotated state\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_rotated_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/devices/default_qubit.py:316\u001b[0m, in \u001b[0;36mDefaultQubit._apply_operation\u001b[0;34m(self, state, operation)\u001b[0m\n\u001b[1;32m    313\u001b[0m     axes \u001b[39m=\u001b[39m [ax \u001b[39m+\u001b[39m shift \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwires\u001b[39m.\u001b[39mindices(wires)]\n\u001b[1;32m    314\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_ops[operation\u001b[39m.\u001b[39mbase_name](state, axes)\n\u001b[0;32m--> 316\u001b[0m matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_asarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_unitary_matrix(operation), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC_DTYPE)\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m operation \u001b[39min\u001b[39;00m diagonal_in_z_basis:\n\u001b[1;32m    319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_diagonal_unitary(state, matrix, wires)\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/devices/default_qubit.py:647\u001b[0m, in \u001b[0;36mDefaultQubit._get_unitary_matrix\u001b[0;34m(self, unitary)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[39mif\u001b[39;00m unitary \u001b[39min\u001b[39;00m diagonal_in_z_basis:\n\u001b[1;32m    645\u001b[0m     \u001b[39mreturn\u001b[39;00m unitary\u001b[39m.\u001b[39meigvals()\n\u001b[0;32m--> 647\u001b[0m \u001b[39mreturn\u001b[39;00m unitary\u001b[39m.\u001b[39;49mmatrix()\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/operation.py:745\u001b[0m, in \u001b[0;36mOperator.matrix\u001b[0;34m(self, wire_order)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatrix\u001b[39m(\u001b[39mself\u001b[39m, wire_order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    726\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Representation of the operator as a matrix in the computational basis.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \n\u001b[1;32m    728\u001b[0m \u001b[39m    If ``wire_order`` is provided, the numerical representation considers the position of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39m        tensor_like: matrix representation\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     canonical_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_matrix(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhyperparameters)\n\u001b[1;32m    747\u001b[0m     \u001b[39mif\u001b[39;00m wire_order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwires \u001b[39m==\u001b[39m Wires(wire_order):\n\u001b[1;32m    748\u001b[0m         \u001b[39mreturn\u001b[39;00m canonical_matrix\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/ops/qubit/parametric_ops_single_qubit.py:622\u001b[0m, in \u001b[0;36mRot.compute_matrix\u001b[0;34m(phi, theta, omega)\u001b[0m\n\u001b[1;32m    619\u001b[0m     s \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mcast_like(qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39masarray(s, like\u001b[39m=\u001b[39minterface), \u001b[39m1\u001b[39mj)\n\u001b[1;32m    621\u001b[0m \u001b[39m# The following variable is used to assert the all terms to be stacked have same shape\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m one \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mones_like(phi) \u001b[39m*\u001b[39;49m qml\u001b[39m.\u001b[39;49mmath\u001b[39m.\u001b[39;49mones_like(omega)\n\u001b[1;32m    623\u001b[0m c \u001b[39m=\u001b[39m c \u001b[39m*\u001b[39m one\n\u001b[1;32m    624\u001b[0m s \u001b[39m=\u001b[39m s \u001b[39m*\u001b[39m one\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/numpy/tensor.py:177\u001b[0m, in \u001b[0;36mtensor.__array_ufunc__\u001b[0;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m# Iterate through the ufunc outputs and convert each to a PennyLane tensor.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m# We also correctly set the requires_grad attribute.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(ufunc_output)):  \u001b[39m# pylint: disable=consider-using-enumerate\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     ufunc_output[i] \u001b[39m=\u001b[39m tensor(ufunc_output[i], requires_grad\u001b[39m=\u001b[39;49mrequires_grad)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ufunc_output) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# the ufunc has a single output so return a single tensor\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m ufunc_output[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/numpy/tensor.py:114\u001b[0m, in \u001b[0;36mtensor.__new__\u001b[0;34m(cls, input_array, requires_grad, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m obj \u001b[39m=\u001b[39m asarray(input_array, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, onp\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 114\u001b[0m     obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49mview(\u001b[39mcls\u001b[39;49m)\n\u001b[1;32m    115\u001b[0m     obj\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m requires_grad\n\u001b[1;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/PhD_Dir/WI4650-2023/qkernel-demo-2023/venv/lib/python3.9/site-packages/pennylane/numpy/tensor.py:119\u001b[0m, in \u001b[0;36mtensor.__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    115\u001b[0m         obj\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m requires_grad\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m--> 119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_finalize__\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m    120\u001b[0m     \u001b[39m# pylint: disable=attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = NesterovMomentumOptimizer(0.01)\n",
    "batch_size = 10\n",
    "\n",
    "# train the variational classifier\n",
    "theta = theta_init\n",
    "bias = bias_init\n",
    "for it in range(n_iters):\n",
    "\n",
    "    # Update the weights for each optimizer step\n",
    "    batch_index = rng.integers(0, len(X_train), size=batch_size)\n",
    "    X_batch = np.array(X_train[batch_index])\n",
    "    y_batch = np.array(y_train[batch_index])\n",
    "    theta, bias, _, _ = opt.step(cost, theta, bias, X_batch, y_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(vqc_with_bias(theta, bias, x)) for x in X_train]\n",
    "    predictions_test = [np.sign(vqc_with_bias(theta, bias, x)) for x in X_test]\n",
    "\n",
    "    # Compute accuracy on train and test set\n",
    "    acc_train = accuracy_score(y_train, predictions_train)\n",
    "    acc_test = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(theta, bias, X_train, y_train), acc_train, acc_test)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ba57309a75c1f2334b12eaf868931394174054c28fafce8b57219f88302837c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('plTutorialsEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
